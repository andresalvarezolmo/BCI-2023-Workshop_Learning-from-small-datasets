{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82952a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from skorch import NeuralNet\n",
    "from skorch.utils import to_numpy\n",
    "from sklearn.base import TransformerMixin\n",
    "from braindecode.models import EEGNetv4\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from moabb.paradigms import MotorImagery\n",
    "from moabb.datasets import Cho2017\n",
    "from moabb.evaluations import WithinSessionEvaluation, CrossSessionEvaluation\n",
    "\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf0f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clf_layers(model: nn.Sequential):\n",
    "    \"\"\"\n",
    "    Remove the classification layers from braindecode models.\n",
    "    Tested on EEGNetv4, Deep4Net (i.e. DeepConvNet), and EEGResNet.\n",
    "    \"\"\"\n",
    "    new_layers = []\n",
    "    for name, layer in model.named_children():\n",
    "        if 'classif' in name:\n",
    "            continue\n",
    "        if 'softmax' in name:\n",
    "            continue\n",
    "        new_layers.append((name, layer))\n",
    "    return nn.Sequential(OrderedDict(new_layers))\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84cc5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenNeuralNetTransformer(NeuralNet, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            criterion=nn.MSELoss,  # should be unused\n",
    "            unique_name=None,  # needed for a unique digest in MOABB\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            criterion=criterion,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.initialize()\n",
    "        self.unique_name = unique_name\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self  # do nothing\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.infer(X)\n",
    "        return to_numpy(X)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return super().__repr__() + self.unique_name\n",
    "    \n",
    "def flatten_batched(X):\n",
    "    return X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a1a5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class RandomLogisticRegression(LogisticRegression):\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        self.coef_ = np.random.randn(\n",
    "            1 if n_classes == 2 else n_classes,\n",
    "            n_features\n",
    "        )\n",
    "        self.intercept_ = np.random.randn(\n",
    "            1 if n_classes == 2 else n_classes\n",
    "        )\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb1a966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/braindecode/models/base.py:23: UserWarning: EEGNetv4: 'in_chans' is depreciated. Use 'n_chans' instead.\n",
      "  warnings.warn(\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/braindecode/models/base.py:23: UserWarning: EEGNetv4: 'n_classes' is depreciated. Use 'n_outputs' instead.\n",
      "  warnings.warn(\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/braindecode/models/base.py:23: UserWarning: EEGNetv4: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# download the model from the hub:\n",
    "path_kwargs = hf_hub_download(\n",
    "    repo_id='PierreGtch/EEGNetv4',\n",
    "    filename='EEGNetv4_Lee2019_MI/kwargs.pkl',\n",
    ")\n",
    "path_params = hf_hub_download(\n",
    "    repo_id='PierreGtch/EEGNetv4',\n",
    "    filename='EEGNetv4_Lee2019_MI/model-params.pkl',\n",
    ")\n",
    "with open(path_kwargs, 'rb') as f:\n",
    "    kwargs = pickle.load(f)\n",
    "module_cls = kwargs['module_cls']\n",
    "module_kwargs = kwargs['module_kwargs']\n",
    "\n",
    "# load the model with pre-trained weights:\n",
    "torch_module = module_cls(**module_kwargs)\n",
    "torch_module.load_state_dict(torch.load(path_params, map_location='cpu'))\n",
    "embedding = freeze_model(remove_clf_layers(torch_module)).double()\n",
    "\n",
    "# Integrate the model in a Scikit-learn pipeline:\n",
    "sklearn_pipeline = Pipeline([\n",
    "    ('embedding', FrozenNeuralNetTransformer(embedding, unique_name='pretrained_Lee2019')),\n",
    "    ('flatten', FunctionTransformer(flatten_batched)),\n",
    "    ('classifier', RandomLogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ec3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm = MotorImagery(\n",
    "    channels=['C3', 'Cz', 'C4'],  # Same as the ones used to pre-train the embedding\n",
    "    events=['left_hand', 'right_hand'],\n",
    "    n_classes=2,\n",
    "    fmin=0.5,\n",
    "    fmax=40,\n",
    "    tmin=0,\n",
    "    tmax=3,\n",
    "    resample=128\n",
    ")\n",
    "datasets = [Cho2017()]\n",
    "evaluation = WithinSessionEvaluation(\n",
    "    paradigm=paradigm,\n",
    "    datasets=datasets,\n",
    "    overwrite=True,\n",
    "    suffix='demo',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21337710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:   0%|          | 0/52 [00:00<?, ?it/s]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 200 events (all good), 0 – 3 s (baseline off), ~7.1 MiB, data loaded,\n",
      " 'left_hand': 100\n",
      " 'right_hand': 100>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:   2%|▏         | 1/52 [00:03<03:16,  3.86s/it]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 200 events (all good), 0 – 3 s (baseline off), ~7.1 MiB, data loaded,\n",
      " 'left_hand': 100\n",
      " 'right_hand': 100>\n",
      "  warn(f\"warnEpochs {epochs}\")\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:   4%|▍         | 2/52 [00:07<02:56,  3.54s/it]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 200 events (all good), 0 – 3 s (baseline off), ~7.1 MiB, data loaded,\n",
      " 'left_hand': 100\n",
      " 'right_hand': 100>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:   6%|▌         | 3/52 [00:10<02:52,  3.51s/it]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 200 events (all good), 0 – 3 s (baseline off), ~7.1 MiB, data loaded,\n",
      " 'left_hand': 100\n",
      " 'right_hand': 100>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:   8%|▊         | 4/52 [00:14<02:49,  3.54s/it]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 200 events (all good), 0 – 3 s (baseline off), ~7.1 MiB, data loaded,\n",
      " 'left_hand': 100\n",
      " 'right_hand': 100>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:  10%|▉         | 5/52 [00:17<02:43,  3.49s/it]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 200 events (all good), 0 – 3 s (baseline off), ~7.1 MiB, data loaded,\n",
      " 'left_hand': 100\n",
      " 'right_hand': 100>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:  12%|█▏        | 6/52 [00:21<02:39,  3.46s/it]Trials demeaned and stacked with zero buffer to create continuous data -- edge effects present\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/moabb/datasets/preprocessing.py:278: UserWarning: warnEpochs <Epochs | 240 events (all good), 0 – 3 s (baseline off), ~8.5 MiB, data loaded,\n",
      " 'left_hand': 120\n",
      " 'right_hand': 120>\n",
      "  warn(f\"warnEpochs {epochs}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hdf5_path provided, models will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cho2017-WithinSession:  13%|█▎        | 7/52 [00:25<02:55,  3.91s/it]Downloading data from 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100295/mat_data/s08.mat' to file '/Users/andresalvarezolmo/mne_data/MNE-gigadb-data/gigadb-datasets/live/pub/10.5524/100001_101000/100295/mat_data/s08.mat'.\n",
      "/Users/andresalvarezolmo/Documents/hume/ACS/playground/BCI-2023-Workshop_Learning-from-small-datasets/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3.ap-northeast-1.wasabisys.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "results = evaluation.process(pipelines=dict(demo_pipeline=sklearn_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy        f1  recall  specificity  precision\n",
      "0     0.515  0.497032   0.515        0.515   0.507688\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "    \"accuracy\": [results['score'].mean()],\n",
    "    \"f1\": [results[\"f1\"].mean()],\n",
    "    \"recall\": [results[\"recall\"].mean()],\n",
    "    \"specificity\": [results[\"specificity\"].mean()],\n",
    "    \"precision\": [results[\"precision\"].mean()]     \n",
    "    } \n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
