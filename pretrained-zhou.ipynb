{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a5dc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from skorch import NeuralNet\n",
    "from skorch.utils import to_numpy\n",
    "from sklearn.base import TransformerMixin\n",
    "from braindecode.models import EEGNetv4\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from moabb.paradigms import MotorImagery\n",
    "from moabb.datasets import Zhou2016\n",
    "from moabb.evaluations import WithinSessionEvaluation, CrossSessionEvaluation\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d109511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clf_layers(model: nn.Sequential):\n",
    "    \"\"\"\n",
    "    Remove the classification layers from braindecode models.\n",
    "    Tested on EEGNetv4, Deep4Net (i.e. DeepConvNet), and EEGResNet.\n",
    "    \"\"\"\n",
    "    new_layers = []\n",
    "    for name, layer in model.named_children():\n",
    "        if 'classif' in name:\n",
    "            continue\n",
    "        if 'softmax' in name:\n",
    "            continue\n",
    "        new_layers.append((name, layer))\n",
    "    return nn.Sequential(OrderedDict(new_layers))\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86c600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenNeuralNetTransformer(NeuralNet, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            criterion=nn.MSELoss,  # should be unused\n",
    "            unique_name=None,  # needed for a unique digest in MOABB\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            criterion=criterion,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.initialize()\n",
    "        self.unique_name = unique_name\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self  # do nothing\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.infer(X)\n",
    "        return to_numpy(X)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return super().__repr__() + self.unique_name\n",
    "    \n",
    "def flatten_batched(X):\n",
    "    return X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e89ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# download the model from the hub:\n",
    "path_kwargs = hf_hub_download(\n",
    "    repo_id='PierreGtch/EEGNetv4',\n",
    "    filename='EEGNetv4_Zhou2016/kwargs.pkl',\n",
    ")\n",
    "path_params = hf_hub_download(\n",
    "    repo_id='PierreGtch/EEGNetv4',\n",
    "    filename='EEGNetv4_Zhou2016/model-params.pkl',\n",
    ")\n",
    "with open(path_kwargs, 'rb') as f:\n",
    "    kwargs = pickle.load(f)\n",
    "    module_cls = kwargs['module_cls']\n",
    "    module_kwargs = kwargs['module_kwargs']\n",
    "\n",
    "# load the model with pre-trained weights:\n",
    "torch_module = module_cls(**module_kwargs)\n",
    "torch_module.load_state_dict(torch.load(path_params, map_location='cpu'))\n",
    "embedding = freeze_model(remove_clf_layers(torch_module)).double()\n",
    "# embedding = remove_clf_layers(torch_module).double()\n",
    "\n",
    "# Integrate the model in a Scikit-learn pipeline:\n",
    "sklearn_pipeline = Pipeline([\n",
    "    ('embedding', FrozenNeuralNetTransformer(embedding, unique_name='pretrained_Zhou2016')),\n",
    "    ('flatten', FunctionTransformer(flatten_batched)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm = MotorImagery(\n",
    "    channels=['C3', 'Cz', 'C4'],  # Same channels as used during pre-training.\n",
    "    events=['left_hand', 'right_hand', 'feet'],\n",
    "    n_classes=3,\n",
    "    fmin=0.5,\n",
    "    fmax=40,\n",
    "    tmin=0,\n",
    "    tmax=3,\n",
    "    resample=128,\n",
    ")\n",
    "datasets = [Zhou2016()]\n",
    "\n",
    "# --------------------------\n",
    "# Load data from MOABB\n",
    "# --------------------------\n",
    "# Note: get_data returns a dictionary with a key per subject.\n",
    "data = paradigm.get_data(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bb4c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y shape to one hot encoding\n",
    "one_hot_y = []\n",
    "i = 0\n",
    "for label in y:\n",
    "    if(y[i] == \"feet\"):\n",
    "        one_hot_y.append(0)\n",
    "    if(y[i] == \"left_hand\"):\n",
    "        one_hot_y.append(1)\n",
    "    if(y[i] == \"right_hand\"):\n",
    "        one_hot_y.append(2)\n",
    "    i+=1\n",
    "\n",
    "one_hot_y = np.array(one_hot_y, dtype=np.int64)\n",
    "\n",
    "X_finetune, X_test, y_finetune, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "63430607",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sklearn_pipeline.fit(X_finetune, y_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a37167e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm = MotorImagery(\n",
    "    channels=['C3', 'Cz', 'C4'],  # Same as the ones used to pre-train the embedding\n",
    "    events=['left_hand', 'right_hand', 'feet'],\n",
    "    n_classes=3,\n",
    "    fmin=0.5,\n",
    "    fmax=40,\n",
    "    tmin=0,\n",
    "    tmax=3,\n",
    "    resample=128\n",
    ")\n",
    "datasets = [Zhou2016()]\n",
    "evaluation = WithinSessionEvaluation(\n",
    "    paradigm=paradigm,\n",
    "    datasets=datasets,\n",
    "    overwrite=True,\n",
    "    suffix='demo',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8142e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation.process(pipelines=dict(demo_pipeline=sklearn_pipeline))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9ad967b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy        f1    recall  specificity  precision\n",
      "0   0.86225  0.861143  0.862278     0.931198   0.873026\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"accuracy\": [results['score'].mean()],\n",
    "    \"f1\": [results[\"f1\"].mean()],\n",
    "    \"recall\": [results[\"recall\"].mean()],\n",
    "    \"specificity\": [results[\"specificity\"].mean()],\n",
    "    \"precision\": [results[\"precision\"].mean()]     \n",
    "    } \n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
